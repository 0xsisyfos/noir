use std::str::FromStr;

use acvm::FieldElement;
use noirc_errors::{Position, Span};

use crate::lexer::token as noir_token;
use crate::lexer::lexer::IntType;
use crate::lexer::errors::LexerErrorKind;

use lalrpop_util::{ErrorRecovery, ParseError};


// TODO: cleanup. is this what we want for ErrorRecovery?
// grammar;
grammar<'err>(errors: &'err mut Vec<ErrorRecovery<usize, Token<'input>, &'static str>>);

extern {
    type Error = Vec<LexerErrorKind>;
}

// TODO: rename Term
pub Term: noir_token::Token = {

    // '\r's are invalid in Noir
    "\r" => noir_token::Token::Invalid('\r'),

    <s:Symbol> => s,
    <n:Num> => noir_token::Token::Int(n),
    <b:Bool> => noir_token::Token::Bool(b),
    <k:Keyword> => noir_token::Token::Keyword(k),

    <x:LineComment> => noir_token::Token::LineComment(x.0, x.1),

    // "(" <t:Term> ")" => t,
};

// TODO: fix string range (add '_')
// TODO: fix maximum size (currently silently truncates too-large literals, see #4631)
Num: FieldElement = <integer_str_raw:r"[0-9]+"> =>? {
    // Underscores needs to be stripped out before the literal can be converted to a `FieldElement.
    let integer_str = integer_str_raw.replace('_', "");

    match FieldElement::try_from_str(&integer_str) {
        None => {
            let error = vec![LexerErrorKind::InvalidIntegerLiteral {
                // TODO: fix span?
                span: Span::empty(0), // inclusive(start, end),
                found: integer_str,
            }];

            // TODO: error recovery
            // errors.push(ParseError::User { error });
            Err(ParseError::User { error })

        }
        Some(integer) => Ok(integer),
    }
};

Bool: bool = {
    "true" => true,
    "false" => false,
};

Keyword: noir_token::Keyword = {
    "as" => noir_token::Keyword::As,
    "assert" => noir_token::Keyword::Assert,
    "assert_eq" => noir_token::Keyword::AssertEq,
    "bool" => noir_token::Keyword::Bool,
    "break" => noir_token::Keyword::Break,
    "call_data" => noir_token::Keyword::CallData,
    "char" => noir_token::Keyword::Char,
    "comptime" => noir_token::Keyword::CompTime,
    "constrain" => noir_token::Keyword::Constrain,
    "continue" => noir_token::Keyword::Continue,
    "contract" => noir_token::Keyword::Contract,
    "crate" => noir_token::Keyword::Crate,
    "dep" => noir_token::Keyword::Dep,
    "distinct" => noir_token::Keyword::Distinct,
    "else" => noir_token::Keyword::Else,
    "Field" => noir_token::Keyword::Field,
    "fn" => noir_token::Keyword::Fn,
    "for" => noir_token::Keyword::For,
    "fmtstr" => noir_token::Keyword::FormatString,
    "global" => noir_token::Keyword::Global,
    "if" => noir_token::Keyword::If,
    "impl" => noir_token::Keyword::Impl,
    "in" => noir_token::Keyword::In,
    "let" => noir_token::Keyword::Let,
    "mod" => noir_token::Keyword::Mod,
    "mut" => noir_token::Keyword::Mut,
    "pub" => noir_token::Keyword::Pub,
    "quote" => noir_token::Keyword::Quote,
    "return" => noir_token::Keyword::Return,
    "return_data" => noir_token::Keyword::ReturnData,
    "str" => noir_token::Keyword::String,
    "struct" => noir_token::Keyword::Struct,
    "trait" => noir_token::Keyword::Trait,
    "type" => noir_token::Keyword::Type,
    "unchecked" => noir_token::Keyword::Unchecked,
    "unconstrained" => noir_token::Keyword::Unconstrained,
    "use" => noir_token::Keyword::Use,
    "where" => noir_token::Keyword::Where,
    "while" => noir_token::Keyword::While,
}

// note: this currently rejects files that end with "//",
// i.e. no terminating "\n".
//
// how to encode EOF in lalrpop? or simply append "\n"
LineComment: (String, Option<noir_token::DocStyle>) = {
    "//\n" => ("//".to_string(), None),
    <comment_str:r"//[^!/\n][^\n]*\n"> => (comment_str.to_string(), None),
    <comment_str:r"//![^\n]*\n"> => (comment_str.to_string(), Some(noir_token::DocStyle::Inner)),
    <comment_str:r"///[^\n]*\n"> => (comment_str.to_string(), Some(noir_token::DocStyle::Outer)),
} 

// // /* comment */
// // /** outer doc block */
// // /*! inner doc block */
// BlockComment: (String, Option<noir_token::DocStyle>) = {
//     "/*" <style:BlockCommentStyle> <comment:InnerBlockComment> "*/" => _
// } 
// 
// InnerBlockComment: _ = {
//     "/*" <style:BlockCommentStyle> <comment:InnerBlockComment> "*/" => _
// 
//     _ => _
// }

// Attribute: noir_token::Attribute = {
//     "#[" <x:AttributeInner> "]" => x,
// }

// TODO:
// AttributeInner: noir_token::Attribute = {
//     r"[^()[]]*" "(" r"[^()[]]*" ")"
// }

Symbol: noir_token::Token = {
    "<" => noir_token::Token::Less,
    "<=" => noir_token::Token::LessEqual,
    ">" => noir_token::Token::Greater,
    ">=" => noir_token::Token::GreaterEqual,
    "==" => noir_token::Token::Equal,
    "!=" => noir_token::Token::NotEqual,
    "+" => noir_token::Token::Plus,
    "-" => noir_token::Token::Minus,
    "*" => noir_token::Token::Star,
    "/" => noir_token::Token::Slash,
    "%" => noir_token::Token::Percent,
    "&" => noir_token::Token::Ampersand,
    "^" => noir_token::Token::Caret,
    "<<" => noir_token::Token::ShiftLeft,
    ">>" => noir_token::Token::ShiftRight,
    "." => noir_token::Token::Dot,
    ".." => noir_token::Token::DoubleDot,
    "(" => noir_token::Token::LeftParen,
    ")" => noir_token::Token::RightParen,
    "{" => noir_token::Token::LeftBrace,
    "}" => noir_token::Token::RightBrace,
    "[" => noir_token::Token::LeftBracket,
    "]" => noir_token::Token::RightBracket,
    "->" => noir_token::Token::Arrow,
    "|" => noir_token::Token::Pipe,
    "#" => noir_token::Token::Pound,
    "," => noir_token::Token::Comma,
    ":" => noir_token::Token::Colon,
    "::" => noir_token::Token::DoubleColon,
    ";" => noir_token::Token::Semicolon,
    "!" => noir_token::Token::Bang,
    "=" => noir_token::Token::Assign,
}


IntType: noir_token::Token = {
    <integer_type:r"(i|u)"> <integer_str:r"[0-9]+"> =>? {
        match integer_str.parse::<u32>() {
            Err(_) => {
                let error = vec![LexerErrorKind::UnexpectedCharacter {
                    // TODO: fix span
                    span: Span::empty(0), // inclusive(start, end),
                    expected: "0..9".to_string(),
                    found: integer_str.chars().next(),
                }];

                // TODO: error recovery
                // errors.push(ParseError::User { error });
                Err(ParseError::User { error })
            }
            Ok(type_size) => {
                if integer_type == "i" {
                    Ok(noir_token::Token::IntType(IntType::Signed(type_size)))
                } else {
                    Ok(noir_token::Token::IntType(IntType::Unsigned(type_size)))
                }
            }
        }
    },

}

String: noir_token::Token = {
    "\"" <s:StringInner> => noir_token::Token::Str(s)
}

StringInner: String = {
    "\\r" <StringInner> => {
        let mut result = "\r".to_owned();
        result.push_str(&<>);
        return result;
    },

    "\\n" <StringInner> => {
        let mut result = "\n".to_owned();
        result.push_str(&<>);
        return result;
    },

    "\\t" <StringInner> => {
        let mut result = "\t".to_owned();
        result.push_str(&<>);
        return result;
    },

    "\\0" <StringInner> => {
        let mut result = "\0".to_owned();
        result.push_str(&<>);
        return result;
    },

    "\\\"" <StringInner> => {
        let mut result = "\"".to_owned();
        result.push_str(&<>);
        return result;
    },

    "\\\\" <StringInner> => {
        let mut result = "\\".to_owned();
        result.push_str(&<>);
        return result;
    },

    <s:r"[^\\]"> <t:StringInner> => {
        let mut result = s;
        result.push_str(&t);
        return result;
    },

    "\"" => "",
}




// TODO: rest of tokens
//
// pub enum Token {
//     Ident(String),
//     Str(String),
//     RawStr(String, u8),
//     FmtStr(String),
//     Attribute(Attribute),
//
//     #[allow(clippy::upper_case_acronyms)]
//     EOF,
// 
//     Whitespace(String),
// 
//     /// An invalid character is one that is not in noir's language or grammar.
//     ///
//     /// We don't report invalid tokens in the source as errors until parsing to
//     /// avoid reporting the error twice (once while lexing, again when it is encountered
//     /// during parsing). Reporting during lexing then removing these from the token stream
//     /// would not be equivalent as it would change the resulting parse.
//     Invalid(char),
// }

